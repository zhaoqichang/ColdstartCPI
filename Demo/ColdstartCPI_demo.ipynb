{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhaoqichang/ColdstartCPI/blob/main/Demo/ColdstartCPI_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA5ncjhFVt8Q"
      },
      "source": [
        "# ColdstartCPI Running Demo\n",
        "\n",
        "| [Open In Colab](https://colab.research.google.com/github/pz-white/DrugBAN/blob/main/drugban_demo.ipynb) (click `Runtime` → `Run all (Ctrl+F9)` |\n",
        "\n",
        "This is a code demo of ColdstartCPI framework for compound-protein interaction prediction. If you don't train the model, it will take about 10 minutes to run the whole pipeline. If you want to train the model, it will cost you 1 hour to train it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZPwj94JXy8K"
      },
      "source": [
        "## Setup\n",
        "\n",
        "The first few blocks of code are necessary to set up the notebook execution environment. This checks if the notebook is running on Google Colab and installs required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i-mTHW00YW9S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e1ff458-ba83-4a07-ec0d-01c2b37d336e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on CoLab\n",
            "\u001b[33mWARNING: Skipping yellowbrick as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: rdkit-pypi in /usr/local/lib/python3.10/dist-packages (2022.9.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi) (9.4.0)\n",
            "Requirement already satisfied: Mol2Vec in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from Mol2Vec) (1.26.4)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from Mol2Vec) (4.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from Mol2Vec) (4.66.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from Mol2Vec) (1.4.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from Mol2Vec) (2.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from Mol2Vec) (3.7.1)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.10/dist-packages (from Mol2Vec) (7.34.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from Mol2Vec) (0.13.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim->Mol2Vec) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->Mol2Vec) (6.4.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from IPython->Mol2Vec) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from IPython->Mol2Vec) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from IPython->Mol2Vec) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from IPython->Mol2Vec) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from IPython->Mol2Vec) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from IPython->Mol2Vec) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from IPython->Mol2Vec) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from IPython->Mol2Vec) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from IPython->Mol2Vec) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from IPython->Mol2Vec) (4.9.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Mol2Vec) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Mol2Vec) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Mol2Vec) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Mol2Vec) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Mol2Vec) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Mol2Vec) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Mol2Vec) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Mol2Vec) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->Mol2Vec) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->Mol2Vec) (2024.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->IPython->Mol2Vec) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->IPython->Mol2Vec) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->Mol2Vec) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->Mol2Vec) (1.16.0)\n",
            "Collecting bio_embeddings\n",
            "  Using cached bio_embeddings-0.1.6-py3-none-any.whl (73 kB)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from bio_embeddings) (1.4.4)\n",
            "Collecting biopython<2.0,>=1.76 (from bio_embeddings)\n",
            "  Using cached biopython-1.83-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "Collecting gensim<4.0.0,>=3.8.2 (from bio_embeddings)\n",
            "  Using cached gensim-3.8.3.tar.gz (23.4 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting h5py<3.0.0,>=2.10.0 (from bio_embeddings)\n",
            "  Using cached h5py-2.10.0.tar.gz (301 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting importlib_metadata<2.0.0,>=1.7.0 (from bio_embeddings)\n",
            "  Using cached importlib_metadata-1.7.0-py2.py3-none-any.whl (31 kB)\n",
            "INFO: pip is looking at multiple versions of bio-embeddings to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting bio_embeddings\n",
            "  Using cached bio_embeddings-0.1.5-py3-none-any.whl (72 kB)\n",
            "  Using cached bio_embeddings-0.1.4-py3-none-any.whl (84 kB)\n",
            "Collecting lock<2019.0.0,>=2018.3.25 (from bio_embeddings)\n",
            "  Using cached lock-2018.3.25.2110.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from bio_embeddings) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.18.3 in /usr/local/lib/python3.10/dist-packages (from bio_embeddings) (1.26.4)\n",
            "Collecting pandas<2.0.0,>=1.0.3 (from bio_embeddings)\n",
            "  Using cached pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "Collecting plotly<5.0.0,>=4.6.0 (from bio_embeddings)\n",
            "  Using cached plotly-4.14.3-py2.py3-none-any.whl (13.2 MB)\n",
            "Collecting ruamel.yaml<0.17.0,>=0.16.10 (from bio_embeddings)\n",
            "  Using cached ruamel.yaml-0.16.13-py2.py3-none-any.whl (111 kB)\n",
            "Collecting scikit-learn<0.23.0,>=0.22.2.post1 (from bio_embeddings)\n",
            "  Using cached scikit-learn-0.22.2.post1.tar.gz (6.9 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from bio_embeddings) (1.11.4)\n",
            "Collecting bio_embeddings\n",
            "  Using cached bio_embeddings-0.1.3-py3-none-any.whl (73 kB)\n",
            "Collecting allennlp<0.10.0,>=0.9.0 (from bio_embeddings)\n",
            "  Using cached allennlp-0.9.0-py3-none-any.whl (7.6 MB)\n",
            "Collecting torch<2.0.0,>=1.5.0 (from bio_embeddings)\n",
            "  Using cached torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.45.0 in /usr/local/lib/python3.10/dist-packages (from bio_embeddings) (4.66.4)\n",
            "Collecting transformers<3.0.0,>=2.8.0 (from bio_embeddings)\n",
            "  Using cached transformers-2.11.0-py3-none-any.whl (674 kB)\n",
            "Collecting umap-learn<0.5.0,>=0.4.2 (from bio_embeddings)\n",
            "  Using cached umap-learn-0.4.6.tar.gz (69 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides (from allennlp<0.10.0,>=0.9.0->bio_embeddings)\n",
            "  Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from allennlp<0.10.0,>=0.9.0->bio_embeddings) (3.8.1)\n",
            "Collecting spacy<2.2,>=2.1.0 (from allennlp<0.10.0,>=0.9.0->bio_embeddings)\n",
            "  Using cached spacy-2.1.9.tar.gz (30.7 MB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Requirement already satisfied: prefetch_generator in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Cloning into 'ColdstartCPI'...\n",
            "remote: Enumerating objects: 1561, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 1561 (delta 11), reused 18 (delta 7), pack-reused 1532\u001b[K\n",
            "Receiving objects: 100% (1561/1561), 2.14 GiB | 27.72 MiB/s, done.\n",
            "Resolving deltas: 100% (681/681), done.\n",
            "Updating files: 100% (1195/1195), done.\n",
            "/content/ColdstartCPI/Demo\n"
          ]
        }
      ],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    print('Running on CoLab')\n",
        "    !pip uninstall --yes yellowbrick\n",
        "    !pip install -U -q psutil\n",
        "    !pip install -U spacy==2.1.0\n",
        "    !pip install rdkit-pypi\n",
        "    !pip install Mol2Vec\n",
        "    !pip install bio_embeddings\n",
        "    !pip install tqdm\n",
        "    !pip install prefetch_generator\n",
        "    !git clone https://github.com/zhaoqichang/ColdstartCPI.git\n",
        "    %cd ColdstartCPI/Demo\n",
        "else:\n",
        "    print('Not running on CoLab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYbEP_eRl2BG"
      },
      "source": [
        "## Import required modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fIJ2vLKhY-Ul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48e4a65b-8d41-4e20-954f-fa65f1fe4ef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.26.4\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import random\n",
        "import os\n",
        "from model import ColdstartCPI\n",
        "from dataset import load_dataset\n",
        "from prefetch_generator import BackgroundGenerator\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "print(np.__version__)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score,precision_recall_curve, auc\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Uldo3zTqO6j"
      },
      "source": [
        "## Configuration\n",
        "\n",
        "For saving time to run a whole pipeline in this demo, we use small subsets, which is located at `Demo/Dataset/demo_data.txt`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hy5zyx55qUtY"
      },
      "outputs": [],
      "source": [
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "Epoch = 100\n",
        "Batch_size = 32\n",
        "Learning_rate = 0.0001\n",
        "Early_stopping_patience = 5\n",
        "save_path = \"./Results/\"\n",
        "if not os.path.exists(save_path):\n",
        "  os.makedirs(save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4O4qY-NrqP7"
      },
      "source": [
        "## Data Loader\n",
        "\n",
        "The train/valid/test datasets are specified using the `CustomDataSet()` function and loaded using the `load_dataset()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N3jilfWtr2VR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fef17b3-ac96-4c1c-84b9-9e3c43bac00b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in the train set:  200\n",
            "Number of samples in the validation set:  200\n",
            "Number of samples in the test set:  200\n"
          ]
        }
      ],
      "source": [
        "train_dataset_load, valid_dataset_load, test_dataset_load = load_dataset(batch_size=Batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pja2a7Ss1-I"
      },
      "source": [
        "## Setup Model and Optimizer\n",
        "\n",
        "Here, we use the previously defined configuration to set up the model and optimizer we will subsequently train.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XXj0Wzw5s-iu"
      },
      "outputs": [],
      "source": [
        "model = ColdstartCPI(unify_num=128,head_num=2).to(device)\n",
        "Loss = nn.CrossEntropyLoss(weight=None)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=Learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9SqUJ5ptTUe"
      },
      "source": [
        "## Model Training and validation\n",
        "\n",
        "Optimize model parameters and check validation performance.\n",
        "It will only cost you 1 hour to train it."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def roc_auc(y,pred):\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "    return roc_auc\n",
        "\n",
        "def pr_auc(y, pred):\n",
        "    precision, recall, thresholds = metrics.precision_recall_curve(y, pred)\n",
        "    pr_auc = metrics.auc(recall, precision)\n",
        "    return pr_auc"
      ],
      "metadata": {
        "id": "sjS6DPZblZbm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_precess(model,pbar,LOSS):\n",
        "    model.eval()\n",
        "    test_losses = []\n",
        "    Y, P, S = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for i, data in pbar:\n",
        "            '''data preparation '''\n",
        "            input_batch, labels = data\n",
        "            labels = labels.to(device)\n",
        "            input_batch = [d.to(device) for d in input_batch]\n",
        "            predicted_scores = model(input_batch)\n",
        "            loss = LOSS(predicted_scores, labels)\n",
        "            correct_labels = labels.to('cpu').data.numpy()\n",
        "            predicted_scores = F.softmax(predicted_scores, 1).to('cpu').data.numpy()\n",
        "            predicted_labels = np.argmax(predicted_scores, axis=1)\n",
        "            predicted_scores = predicted_scores[:, 1]\n",
        "\n",
        "            Y.extend(correct_labels)\n",
        "            P.extend(predicted_labels)\n",
        "            S.extend(predicted_scores)\n",
        "            test_losses.append(loss.item())\n",
        "    Precision = precision_score(Y, P)\n",
        "    Reacll = recall_score(Y, P)\n",
        "    F1_score = f1_score(Y, P)\n",
        "    # AUC = roc_auc_score(Y, S)\n",
        "    AUC = roc_auc(Y,S)\n",
        "    tpr, fpr, _ = precision_recall_curve(Y, S)\n",
        "    # PRC = auc(fpr, tpr)\n",
        "    PRC = pr_auc(Y,S)\n",
        "    Accuracy = accuracy_score(Y, P)\n",
        "    test_loss = np.average(test_losses)\n",
        "    return Y, P, test_loss, Accuracy, Precision, Reacll, F1_score, AUC, PRC"
      ],
      "metadata": {
        "id": "9DKBt14slbVT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(dataset_load, LOSS):\n",
        "    test_pbar = tqdm(\n",
        "        enumerate(\n",
        "            BackgroundGenerator(dataset_load)),\n",
        "        total=len(dataset_load))\n",
        "    T, P, loss_test, Accuracy_test, Precision_test, Recall_test, F1_score_test, AUC_test, PRC_test = \\\n",
        "        test_precess(model,test_pbar, LOSS)\n",
        "    results = 'Loss:{:.5f};Accuracy:{:.5f};Precision:{:.5f};Recall:{:.5f};F1 score:{:.5f};AUC:{:.5f};PRC:{:.5f}.' \\\n",
        "        .format(loss_test, Accuracy_test, Precision_test, Recall_test, F1_score_test, AUC_test, PRC_test)\n",
        "    print(results)\n",
        "    return results,loss_test, Accuracy_test, Precision_test, Recall_test, F1_score_test, AUC_test, PRC_test"
      ],
      "metadata": {
        "id": "Pw5qHGLBlihU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XH9k2L94tXac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3cc3554-2a0b-4182-85e2-feb24a33eedc",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:51<00:00,  4.46s/it]\n",
            "100%|██████████| 25/25 [00:26<00:00,  1.06s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[  1/100] patience: 0 train_loss: 0.69358 valid_loss: 0.67396 valid_AUC: 0.69680 valid_AUPR: 0.71302 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:37<00:00,  3.90s/it]\n",
            "100%|██████████| 25/25 [00:24<00:00,  1.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[  2/100] patience: 0 train_loss: 0.68502 valid_loss: 0.66262 valid_AUC: 0.70120 valid_AUPR: 0.71435 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:19<00:00,  3.16s/it]\n",
            "100%|██████████| 25/25 [00:30<00:00,  1.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[  3/100] patience: 0 train_loss: 0.66849 valid_loss: 0.64993 valid_AUC: 0.74600 valid_AUPR: 0.74160 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 25/25 [01:17<00:00,  3.10s/it]\n",
            "100%|██████████| 25/25 [00:25<00:00,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[  4/100] patience: 0 train_loss: 0.66480 valid_loss: 0.62706 valid_AUC: 0.79940 valid_AUPR: 0.78644 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 25/25 [01:26<00:00,  3.44s/it]\n",
            "100%|██████████| 25/25 [00:22<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[  5/100] patience: 0 train_loss: 0.63424 valid_loss: 0.58244 valid_AUC: 0.84150 valid_AUPR: 0.83059 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:28<00:00,  3.54s/it]\n",
            "100%|██████████| 25/25 [00:22<00:00,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[  6/100] patience: 0 train_loss: 0.59485 valid_loss: 0.56669 valid_AUC: 0.84730 valid_AUPR: 0.83869 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 25/25 [01:17<00:00,  3.10s/it]\n",
            "100%|██████████| 25/25 [00:21<00:00,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[  7/100] patience: 0 train_loss: 0.59688 valid_loss: 0.51810 valid_AUC: 0.87350 valid_AUPR: 0.87035 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 25/25 [01:16<00:00,  3.04s/it]\n",
            "100%|██████████| 25/25 [00:21<00:00,  1.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[  8/100] patience: 0 train_loss: 0.57454 valid_loss: 0.51766 valid_AUC: 0.90540 valid_AUPR: 0.90515 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:15<00:00,  3.03s/it]\n",
            "100%|██████████| 25/25 [00:21<00:00,  1.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[  9/100] patience: 0 train_loss: 0.49304 valid_loss: 0.41695 valid_AUC: 0.93690 valid_AUPR: 0.93641 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:12<00:00,  2.89s/it]\n",
            "100%|██████████| 25/25 [00:23<00:00,  1.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[ 10/100] patience: 0 train_loss: 0.44953 valid_loss: 0.36855 valid_AUC: 0.95940 valid_AUPR: 0.95602 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:13<00:00,  2.93s/it]\n",
            "100%|██████████| 25/25 [00:20<00:00,  1.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[ 11/100] patience: 0 train_loss: 0.40788 valid_loss: 0.33729 valid_AUC: 0.96250 valid_AUPR: 0.96781 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:13<00:00,  2.93s/it]\n",
            "100%|██████████| 25/25 [00:26<00:00,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[ 12/100] patience: 0 train_loss: 0.36511 valid_loss: 0.27949 valid_AUC: 0.97520 valid_AUPR: 0.97799 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 25/25 [01:19<00:00,  3.19s/it]\n",
            "100%|██████████| 25/25 [00:24<00:00,  1.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[ 13/100] patience: 0 train_loss: 0.35555 valid_loss: 0.22040 valid_AUC: 0.98980 valid_AUPR: 0.98970 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:13<00:00,  2.96s/it]\n",
            "100%|██████████| 25/25 [00:20<00:00,  1.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[ 14/100] patience: 0 train_loss: 0.26124 valid_loss: 0.17287 valid_AUC: 0.99740 valid_AUPR: 0.99743 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:14<00:00,  2.97s/it]\n",
            "100%|██████████| 25/25 [00:20<00:00,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[ 15/100] patience: 0 train_loss: 0.20725 valid_loss: 0.14185 valid_AUC: 0.99780 valid_AUPR: 0.99771 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 25/25 [01:15<00:00,  3.02s/it]\n",
            "100%|██████████| 25/25 [00:21<00:00,  1.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[ 16/100] patience: 0 train_loss: 0.22997 valid_loss: 0.12860 valid_AUC: 0.99880 valid_AUPR: 0.99879 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:19<00:00,  3.17s/it]\n",
            "100%|██████████| 25/25 [00:21<00:00,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[ 17/100] patience: 0 train_loss: 0.17457 valid_loss: 0.16494 valid_AUC: 0.99920 valid_AUPR: 0.99920 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 25/25 [01:12<00:00,  2.90s/it]\n",
            "100%|██████████| 25/25 [00:23<00:00,  1.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[ 18/100] patience: 0 train_loss: 0.13845 valid_loss: 0.14355 valid_AUC: 0.98800 valid_AUPR: 0.98501 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:23<00:00,  3.35s/it]\n",
            "100%|██████████| 25/25 [00:22<00:00,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[ 19/100] patience: 1 train_loss: 0.14915 valid_loss: 0.10569 valid_AUC: 0.99920 valid_AUPR: 0.99921 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 25/25 [01:12<00:00,  2.91s/it]\n",
            "100%|██████████| 25/25 [00:22<00:00,  1.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[ 20/100] patience: 0 train_loss: 0.14454 valid_loss: 0.07015 valid_AUC: 0.99940 valid_AUPR: 0.99940 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:21<00:00,  3.26s/it]\n",
            "100%|██████████| 25/25 [00:23<00:00,  1.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[ 21/100] patience: 0 train_loss: 0.07480 valid_loss: 0.05497 valid_AUC: 1.00000 valid_AUPR: 1.00000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:18<00:00,  3.14s/it]\n",
            "100%|██████████| 25/25 [00:28<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[ 22/100] patience: 0 train_loss: 0.05307 valid_loss: 0.04542 valid_AUC: 1.00000 valid_AUPR: 1.00000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:12<00:00,  2.90s/it]\n",
            "100%|██████████| 25/25 [00:24<00:00,  1.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[ 23/100] patience: 1 train_loss: 0.05648 valid_loss: 0.03046 valid_AUC: 1.00000 valid_AUPR: 1.00000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:19<00:00,  3.18s/it]\n",
            "100%|██████████| 25/25 [00:22<00:00,  1.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[ 24/100] patience: 2 train_loss: 0.09718 valid_loss: 0.12893 valid_AUC: 0.99920 valid_AUPR: 0.99923 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:23<00:00,  3.33s/it]\n",
            "100%|██████████| 25/25 [00:24<00:00,  1.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[ 25/100] patience: 3 train_loss: 0.06685 valid_loss: 0.03947 valid_AUC: 0.99960 valid_AUPR: 0.99960 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:15<00:00,  3.04s/it]\n",
            "100%|██████████| 25/25 [00:23<00:00,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[ 26/100] patience: 4 train_loss: 0.02975 valid_loss: 0.01968 valid_AUC: 1.00000 valid_AUPR: 1.00000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "patience = 0\n",
        "best_score = 0\n",
        "best_epoch = 0\n",
        "\"\"\"Start training.\"\"\"\n",
        "print('Training...')\n",
        "epoch_len = len(str(Epoch))\n",
        "for epoch in range(Epoch):\n",
        "  trian_pbar = tqdm(\n",
        "      enumerate(\n",
        "          BackgroundGenerator(train_dataset_load)),\n",
        "      total=len(train_dataset_load))\n",
        "  \"\"\"train\"\"\"\n",
        "  train_losses_in_epoch = []\n",
        "  model.train()\n",
        "  for trian_i, train_data in trian_pbar:\n",
        "    '''data preparation '''\n",
        "    input_batch, trian_labels = train_data\n",
        "    input_batch = [d.to(device) for d in input_batch]\n",
        "    trian_labels = trian_labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    predicted_interaction = model(input_batch)\n",
        "    train_loss = Loss(predicted_interaction, trian_labels)\n",
        "    train_losses_in_epoch.append(train_loss.item())\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss_a_epoch = np.average(train_losses_in_epoch)\n",
        "    \"\"\"valid\"\"\"\n",
        "  valid_pbar = tqdm(\n",
        "      enumerate(\n",
        "          BackgroundGenerator(valid_dataset_load)),\n",
        "      total=len(valid_dataset_load))\n",
        "  _,_,valid_loss_a_epoch, _, _, _, _, AUC_dev, PRC_dev = test_precess(model,valid_pbar,Loss)\n",
        "  valid_score = AUC_dev + PRC_dev\n",
        "  print_msg = (f'[{epoch + 1:>{epoch_len}}/{Epoch:>{epoch_len}}] ' +\n",
        "                  f'patience: {patience} ' +\n",
        "                  f'train_loss: {train_loss_a_epoch:.5f} ' +\n",
        "                  f'valid_loss: {valid_loss_a_epoch:.5f} ' +\n",
        "                  f'valid_AUC: {AUC_dev:.5f} ' +\n",
        "                  f'valid_AUPR: {PRC_dev:.5f} '\n",
        "                  )\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(print_msg)\n",
        "  if valid_score > best_score:\n",
        "    best_score = valid_score\n",
        "    patience = 0\n",
        "    best_epoch = epoch + 1\n",
        "    torch.save(model.state_dict(), save_path + 'valid_best_checkpoint.pth')\n",
        "  else:\n",
        "    patience += 1\n",
        "    if patience == Early_stopping_patience:\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Test the best model\"\"\"\n",
        "print('load trained model...')\n",
        "model.load_state_dict(torch.load(save_path + 'valid_best_checkpoint.pth'))\n",
        "\n",
        "trainset_test_results, Loss_train, Accuracy_train, Precision_train, Recall_train, F1_score_train, AUC_train, PRC_train = test_model(train_dataset_load, Loss)\n",
        "with open(save_path + 'results.txt', 'a') as f:\n",
        "  f.write(\"The result of train set:\"+ trainset_test_results + '\\n')\n",
        "\n",
        "testset_test_results, Loss_test, Accuracy_test, Precision_test, Recall_test, F1_score_test, AUC_test, PRC_test = test_model(test_dataset_load, Loss)\n",
        "with open(save_path + 'results.txt', 'a') as f:\n",
        "  f.write(\"The result of test set:\" + testset_test_results + '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZO7v21mygCF",
        "outputId": "b60dde4c-b55c-4fb7-b2cd-9b80c494df3f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load trained model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:22<00:00,  1.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:0.05497;Accuracy:0.99000;Precision:1.00000;Recall:0.98000;F1 score:0.98990;AUC:1.00000;PRC:1.00000.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:23<00:00,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:0.05497;Accuracy:0.99000;Precision:1.00000;Recall:0.98000;F1 score:0.98990;AUC:1.00000;PRC:1.00000.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expected Output\n",
        "\n",
        "Awesome! You complete all demo steps and should get output like the following. Please note that these numbers might be different due to the update of environment setup on colab.\n",
        "\n",
        "```\n",
        "load trained model...\n",
        "100%|███████████████████████████████████████████| 13/13 [00:02<00:00,  5.80it/s]\n",
        "Loss:0.05332;Accuracy:0.99500;Precision:0.99010;Recall:1.00000;F1 score:0.99502;AUC:1.00000;PRC:1.00000.\n",
        "100%|███████████████████████████████████████████| 13/13 [00:01<00:00,  7.23it/s]\n",
        "Loss:0.05443;Accuracy:0.99500;Precision:0.99010;Recall:1.00000;F1 score:0.99502;AUC:1.00000;PRC:1.00000.\n",
        "```\n",
        "\n",
        "Finally, the output result is saved in the colab temporary directory: `/content/ColdstartCPI/Demo/Results`. You can access it by clicking `Files` tab on the left side of colab interface."
      ],
      "metadata": {
        "id": "haKH889bNPvE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T9tBqlD80Rgf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}